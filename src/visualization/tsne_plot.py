from sklearn.manifold import TSNE  # imports t-SNE to visualize embeddings
import matplotlib.pyplot as plt  # allows for plotting
from mpl_toolkits.mplot3d import Axes3D  # allows for 3D plotting
import matplotlib.cm as cm  # allows for coloring data points
import numpy as np  # imports numpy for matrix operations

from os.path import exists  # check that path exists
from os import mkdir, getcwd  # directory operations
from shutil import rmtree  # remove specific directory contents

import constants as C # constants


class TsnePlot():

    def reduce_clusters(self, clusters: [[int]]) -> [[int]]:
        """
        Creates the reduced-dimension tSNE model
        :param clusters: a vector of similarity scores
        :return reduced cluster model
        """
        # hyperparameters
        perplexity, components = C.C_PERPLEXITY, C.C_COMPONENTS
        iterations, learning_rate = C.C_ITER, C.C_ETA

        # turn clusters into numpy array
        clusters = np.array(clusters)
        n, m, k = clusters.shape

        # Setup for visualizing 2-Dimensional cluster model
        model = TSNE(perplexity=perplexity, n_components=components, init='pca',
                     n_iter=iterations, learning_rate=learning_rate)
        return np.array(model.fit_transform(clusters.reshape(n * m, k))).reshape(n, m, 2)

    def visualize_clusters(self, embeddings: [[int, int]], words: [str], keys: [str],
                           plot_title: str, img_title: str):
        """
        Create a 2-dimensional visual of the embeddings
        :param embeddings: the embedding coordinates
        :param words: the word labels
        :param keys: the original cluster keys
        :param plot_title: the plot title
        :param img_title: the title for saving the image
        """
        plt.figure(figsize=(16, 9))

        cmap = cm.rainbow(np.linspace(0.0, 1.0, len(keys)))
        word_count = 0

        for key, coords, color in zip(keys, embeddings, cmap):
            x, y = coords[:, 0], coords[:, 1]
            plt.scatter(x, y, color=color, alpha=0.9, label=key)

            cluster = len(x)
            for i in range(cluster):
                plt.annotate(words[word_count],
                             xy=(x[i], y[i]),
                             alpha=0.8,
                             size=9.5,
                             textcoords='offset pixels', xytext=(-15, 7),
                             ha='right', va='top')
                word_count += 1

        plt.title(plot_title)
        plt.grid(True)
        plt.legend(loc=4)
        self.save_plot(img_title)
        plt.show()

    def process_vectors(self, word_vectors):
        """
        Return the tokens and words
        :param word_vectors: the word vectors generated by word2vec
        :return: the words and tokens in the vector
        """
        tokens, words = [], []

        # If testing, use a subset of the embeddings to reduce processing time
        if C.TEST:
            count = 0
            test_amount = C.REDUCED_SIZE  # set the number of embeddings that are used
            for word in word_vectors.vocab:
                count += 1
                if count == test_amount:
                    break
                tokens.append(word_vectors[word])
                words.append(word)
        # If not testing, use the entire dataset
        else:
            # tokenize the word embeddings
            for word in word_vectors.vocab:
                tokens.append(word_vectors[word])
                words.append(word)

        return words, tokens

    def reduce_model(self, tokens: [[int]], perplexity: int = 5, components: int = 2,
                     iterations: int = 2400, learning_rate: int = 200) -> [[int]]:
        """
        Reduce the provided model using specified parameters
        :param tokens: the dimensions for each word
        :param perplexity: the number of nearest neighbors
        :param components: the dimension to reduce to
        :param iterations: the number of iterations to run
        :param learning_rate: the step rate
        :return reduced model
        """
        # hyperparameters
        perplexity, components = perplexity, components
        iterations, learning_rate = iterations, learning_rate

        # Setup for visualizing 2-Dimensional model
        model = TSNE(perplexity=perplexity, n_components=components, init='pca',
                     n_iter=iterations, learning_rate=learning_rate)

        return model.fit_transform(tokens)

    def visualize_embeddings_3D(self, embeddings: [[int, int]], labels: [str], plot_title: str, img_title: str):
        """
        Plots the parameterized data into a visual plot
        :param embeddings: the x, y and z datapoints for the embeddings
        :param labels: word labels associated with each xy data point
        :param plot_title: plot title
        :param img_title: title for saved image
        note: this implementation was used as an intial guide for 3D plotting
        https://towardsdatascience.com/google-news-and-leo-tolstoy-
        visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d
        """
        xs, ys, zs = embeddings[:, 0], embeddings[:, 1], embeddings[:, 2]
        total = len(xs)

        plt.figure(figsize=(16, 9))
        fig = plt.figure()
        ax = Axes3D(fig)

        print(f'3D Word Embeddings generating for {total} embeddings...')
        ax.scatter3D(xs, ys, zs, color='#00b4be', edgecolors='#00a3ac')

        total_count = np.arange(0, total, dtype=int)
        for x, y, z, count in zip(xs, ys, zs, total_count):
            if count % C.RESTRICT_3D_TEXT == 0:  # restricts number of annotations
                print(f'count: {count} with label: {labels[count]}')
                ax.text(x, y, z, labels[count], 'y', fontsize=8, color='#000000')

        plt.title(plot_title, y=1.02)
        self.save_plot(img_title)
        plt.show()

    def visualize_embeddings_2D(self, embeddings: [[int, int]], labels: [str], plot_title: str, img_title: str):
        """
        Plots the parameterized data into a visual plot
        :param embeddings: the x and y datapoints for the embeddings
        :param labels: word labels associated with each xy data point
        :param plot_title: plot title
        :param img_title: title for saved image
        """
        x, y = embeddings[:, 0], embeddings[:, 1]
        total = len(x)
        x_min, x_max = min(x) - 10, max(x) + 10

        plt.figure(figsize=(16, 9))
        fig, ax = plt.subplots()

        ax.scatter(x, y, alpha=0.4, color='#0375fa', edgecolors='#0256bc')
        for coord in range(total):
            ax.annotate(labels[coord],
                        xy=(x[coord], y[coord]),
                        size=6,
                        alpha=1,
                        textcoords='offset pixels', xytext=(-8, 5),
                        ha='right', va='top')

        plt.grid(True)
        plt.title(plot_title)
        self.save_plot(img_title)
        plt.show()

    def save_title(self, perplexity: int, components: int, iterations: int, learning_rate: int, extra: str = ''):
        """
        Returns string title for image to be saved with parameter details
        :param perplexity: hyperparameter detailing nearest neighbors
        :param components: reduced dimensions
        :param iterations: total itereations
        :param learning_rate: the step rate for learning
        :param extra: any extra details in string type
        :return: the title for saving the image
        """
        return 'tSNE2D_perplexity' + str(perplexity) + \
               '_components' + str(components) + \
               '_iter' + str(iterations) + \
               '_eta' + str(learning_rate) + extra

    def similarity_clusters(self, word_vectors, cluster_keys: [str]) -> ([str], [[int]]):
        """
        Provide specific cluster keys around which to find similarity clusters
        :param word_vectors: vectors generated from word2vec
        :param cluster_keys: the key words for finding similar word clusters
        :return: the words and embedding clusters
        """
        words, embedding_clusters = [], []

        for word in cluster_keys:
            embeddings = []
            for similar_word, similarity in word_vectors.most_similar(word, topn=C.TOP_N):
                words.append(similar_word)
                embeddings.append(word_vectors[word])
            embedding_clusters.append(embeddings)

        return words, embedding_clusters

    def save_plot(self, title: str):
        """
        Saves plot to specified directory.
        :param title: title of plot
        """
        path = C.GRAPH_DIR

        if C.DELETE_GRAPHS:
            # clear directory if specified by function call
            if exists(path):
                rmtree(path)

        # create directory if it doesn't exist
        if not exists(path):
            mkdir(path)

        # save plot to specified directory
        img = path + title + '.png'
        plt.savefig(img)
